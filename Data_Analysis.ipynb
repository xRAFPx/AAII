{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_AAII.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1kTjSp5Vgh5MXZExZFj8XK-dsy-Cpb2Xd",
      "authorship_tag": "ABX9TyODwIT8K/vhJd4oP0A500ni",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xRAFPx/AAII/blob/master/Data_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-eNfQdVTcm1",
        "colab_type": "code",
        "outputId": "7905f626-589d-4ba8-8154-367095a64a54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPYGLCcsVMha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t0tyBWTGLSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf AAII"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LfOoj-HFcwR",
        "colab_type": "code",
        "outputId": "347de10a-899b-4ee2-ad78-ca85007a3066",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git init"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reinitialized existing Git repository in /content/.git/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDlC4YO2S_fI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import json\n",
        "import csv\n",
        "import string\n",
        "import io\n",
        "import pandas as pd\n",
        "from tempfile import NamedTemporaryFile\n",
        "import shutil\n",
        "from datetime import date\n",
        "import datetime\n",
        "import os.path\n",
        "from os import path\n",
        "import re\n",
        "import xlrd\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from getpass import getpass\n",
        "import geopandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO_Q7ftMnI9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder = '/content/drive/My Drive/Colab Notebooks/AII Project'\n",
        "gitFolder = '/content/AAII/Data'\n",
        "datasetCOVID = 'https://raw.githubusercontent.com/dssg-pt/covid19pt-data/master/data.csv'\n",
        "datasetDeaths = folder + '/Portugal_Deaths.xlsx'\n",
        "dicoAPI = \"https://api.ipma.pt/open-data/distrits-islands.json\"\n",
        "climateAPI = 'https://api.ipma.pt/open-data/observation/climate/'\n",
        "mapPath = folder + 'portugal_continental.shp'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LUZ_-rZOvuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initGit():\n",
        "  !git config --global user.email \"pg41098@alunos.uminho.pt\"\n",
        "  !git config --global user.name \"xRAFPx\"\n",
        "  password = getpass('Password:')\n",
        "  !git clone https://xRAFPx:$password@github.com//xRAFPx/AAII\n",
        "  !git remote set-url origin https://xRAFPx:$password@github.com/xRAFPx/repository.git\n",
        "  !git remote -v\n",
        "  %cd AAII"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N72d3RpWOwhW",
        "colab_type": "code",
        "outputId": "7187046f-20cb-49c0-e661-2e2005781d81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "initGit()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Password:··········\n",
            "Cloning into 'AAII'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 27 (delta 5), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (27/27), done.\n",
            "/content/AAII\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLzhzYEImP21",
        "colab_type": "code",
        "outputId": "28246a49-cc47-44ff-fa71-b0d0445ea14d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if path.exists(folder):\n",
        "  print(\"No directory error\")\n",
        "else:\n",
        "  #Create a new directory (a folder) in your Drive\n",
        "  os.mkdir(folder)\n",
        "  print(\"Directory created\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No directory error\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfkKRJZUstuf",
        "colab_type": "text"
      },
      "source": [
        "# GET IPMA INFORMATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJbMVgJVPJ3j",
        "colab_type": "text"
      },
      "source": [
        "A API do IPMA permite obter os dados dos últimos 2 meses pelo que alguns dos dados, à medida que o tempo for passando, vão ser perdidos. Assim sendo, a solução encontrada foi guardar os dados diários dentro de um só documento (cityCSV) que será atualizado sempre que houver novos dados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2r3mgm7ui9V",
        "colab_type": "text"
      },
      "source": [
        "Imprime o ficheiro JSON na sua forma estruturada para uma melhor leitura dos dados\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5sKBtywTHNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jprint(obj):\n",
        "    # create a formatted string of the Python JSON object\n",
        "    text = json.dumps(obj, sort_keys=True, indent=4)\n",
        "    print(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfzUO_QYuy5n",
        "colab_type": "text"
      },
      "source": [
        "Obtém o número DICO (Identificador único de concelho de acordo com a CAOP -DGT) para cada local (freguesia) definida. A API do IPMA está apenas disponível para Portugal Continental.\n",
        "Retorna\n",
        "*   dico: lista com o nome da freguesia e o valor do seu identificador (DICO), dados necessários para a chamada à API do IPMA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX24Wq89iMFG",
        "colab_type": "text"
      },
      "source": [
        "Tendo em consideração as regiões CCDR (CCDR Norte, CCDR Centro, CCDR Lisboa e Vale do Tejo, CCDR Alentejo e CCDR Algarve)\n",
        "[Link para o mapa indicativo de cada região](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c7/Area_atua%C3%A7%C3%A3o_CCDR.png/800px-Area_atua%C3%A7%C3%A3o_CCDR.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjQ5RxSoTOL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getDico():\n",
        "  response = requests.get(dicoAPI)\n",
        "  # print(response.status_code)\n",
        "  # jprint(response.json())\n",
        "\n",
        "  #Évora nesta API está escrita como \\u00c9vora\n",
        "  local = ['Porto', 'Lisboa', 'Faro', '\\u00c9vora', 'Coimbra']\n",
        "  dico = list()\n",
        "\n",
        "  for x in response.json()['data']:\n",
        "      for l in local:\n",
        "          if x['local'] == l:\n",
        "              #Remove o acento (´) do E de Évora\n",
        "              local_regex = re.sub('[^A-Za-z]+', 'e', l)\n",
        "              dico.append([local_regex.lower(),\"{0:0=2d}\".format(int(x['idDistrito'])) + str(\"{0:0=2d}\".format(int(x['idConcelho'])))])\n",
        "  return dico"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcmk91IZuHwc",
        "colab_type": "text"
      },
      "source": [
        "Esta função obtém a diferença de dias entre o documento com os dados da temperatura (cityCSV) e os dados obtidos a partir da API. Se houver uma diferença (>0), vai adicionar os dados em falta ao documento.\n",
        "\n",
        "*   lastDay: o último dia da chamada da API. Este valor indica o último dia disponível na informação disponibilizada na chamada. Este dia será usado para descobrir a quantidade dos dados em falta. A diferença entre este dia e o último dia do documento que pretendemos que tenha os dados completos será o número de dias em falta no mesmo. \n",
        "*   cityCSV: o caminho do ficheiro.\n",
        "*   lastData: todos os dados resultantes da chamada à API.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-_Kztm1TEP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def countDiffDates(lastDay, cityCSV, lastData):\n",
        "    cw = open(cityCSV, 'r')\n",
        "    listCity = list(cw)\n",
        "    dateList = list()\n",
        "    for row in listCity:\n",
        "        date = row.split(',')[0]\n",
        "        dateList.append(date)\n",
        "    cw.close()  \n",
        "\n",
        "    date_objectNew = datetime.datetime.strptime(lastDay, '%Y-%m-%d').date()\n",
        "    date_object = datetime.datetime.strptime(dateList[-1], '%Y-%m-%d').date()\n",
        "\n",
        "    diff = date_objectNew - date_object\n",
        "    diffDays = diff.days\n",
        "    print(\"Days Missing:\", diffDays)\n",
        "    if diffDays != 0:\n",
        "        res = lastData[-diffDays:]\n",
        "        # Open file in append mode\n",
        "        f = open(cityCSV, 'a', newline='')\n",
        "        for item in res:\n",
        "          # Add contents of list as last row in the csv file\n",
        "          f.write(','.join([str(x) for x in item]) + '\\n')\n",
        "        f.close()\n",
        "        #print(res)\n",
        "    else:\n",
        "      print(\"Data is Updated\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hE9JGnwvKdH",
        "colab_type": "text"
      },
      "source": [
        "Obtém a informação do IPMA para cada local. A chamada da API é formatada para a estrutura da mesma. O método getDiffDates é utilizado.\n",
        "\n",
        "*   lastDay: o último dia da chamada da API.\n",
        "*   cityCSV: o caminho do ficheiro.\n",
        "*   my_list: todos os dados resultantes da chamada à API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjk2Zyh6S8WR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getIPMAData(dico):\n",
        "  dictURL = {1: ['temperature-min','mtnmn'], 2: ['temperature-max', 'mtxmx']} \n",
        "  today = date.today()\n",
        "  for d in range(len(dico)):\n",
        "      print(\"_______________\\n\" + str(dico[d][0]).upper() + \"\\n_______________\")\n",
        "      for i in dictURL:\n",
        "          url = climateAPI + dictURL[i][0] + '/' + dico[d][0] + '/' + dictURL[i][1] + '-' + dico[d][1] + '-' + dico[d][0] + '.csv'\n",
        "          print('\\n' + str(dictURL[i][0]))\n",
        "          #print('\\n' + url) \n",
        "          cityCSV = folder + '/' + str(dico[d][0]) + '_' + dictURL[i][0] + '.csv' \n",
        "          #cityCSV = gitFolder + '/' + str(dico[d][0]) + '_' + dictURL[i][0] + '.csv' \n",
        "          #cityCSV_data = gitFolder + '/DailyWeatherData/' + str(today) + '_' + str(dico[d][0]) + '_' + dictURL[i][0] + '.csv' \n",
        "          #cityCSV_data = folder + '/Daily Data/' + str(today) + '_' + str(dico[d][0]) + '_' + dictURL[i][0] + '.csv' \n",
        "          #print(cityCSV)\n",
        "          with requests.Session() as s:\n",
        "              download = s.get(url)\n",
        "              print(\"Request Status:\", download.status_code)\n",
        "              decoded_content = download.content.decode('utf-8')\n",
        "              cr = csv.reader(decoded_content.splitlines(), delimiter=',')\n",
        "              my_list = list(cr)\n",
        "              lastDay = my_list[-1][0]\n",
        "              cityCSV_data = folder + '/Daily Data/' + lastDay + '_' + str(dico[d][0]) + '_' + dictURL[i][0] + '.csv' \n",
        "              if path.exists(cityCSV):\n",
        "                print(\"Original file already exists\")\n",
        "                print(\"Creating validation file for \", str(today))\n",
        "                with open(cityCSV_data, mode = 'w', newline='') as result_file:\n",
        "                  wrD = csv.writer(result_file, dialect='excel', delimiter=',', quoting=csv.QUOTE_NONE, quotechar='', escapechar=' ')\n",
        "                  wrD.writerows(my_list)\n",
        "              else:\n",
        "                with open(cityCSV, mode = 'w', newline='') as result_file:\n",
        "                  wr = csv.writer(result_file, dialect='excel', delimiter=',', quoting=csv.QUOTE_NONE, quotechar='', escapechar=' ')\n",
        "                  wr.writerows(my_list)\n",
        "              countDiffDates(lastDay, cityCSV, my_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8FGGSUHkK_v",
        "colab_type": "code",
        "outputId": "990c77db-bbb0-4fb6-97ef-e79ab166a8cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dico = list()\n",
        "dico = getDico()\n",
        "print(dico)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['coimbra', '0603'], ['evora', '0705'], ['faro', '0805'], ['lisboa', '1106'], ['porto', '1312']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ewkHy-_kTup",
        "colab_type": "code",
        "outputId": "c19ecfeb-15b5-48db-bb9a-f13138a5a44a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "getIPMAData(dico)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_______________\n",
            "COIMBRA\n",
            "_______________\n",
            "\n",
            "temperature-min\n",
            "Request Status: 200\n",
            "Original file already exists\n",
            "Creating validation file for  2020-05-04\n",
            "Days Missing: 0\n",
            "Data is Updated\n",
            "\n",
            "temperature-max\n",
            "Request Status: 200\n",
            "Original file already exists\n",
            "Creating validation file for  2020-05-04\n",
            "Days Missing: 0\n",
            "Data is Updated\n",
            "_______________\n",
            "EVORA\n",
            "_______________\n",
            "\n",
            "temperature-min\n",
            "Request Status: 200\n",
            "Original file already exists\n",
            "Creating validation file for  2020-05-04\n",
            "Days Missing: 0\n",
            "Data is Updated\n",
            "\n",
            "temperature-max\n",
            "Request Status: 200\n",
            "Original file already exists\n",
            "Creating validation file for  2020-05-04\n",
            "Days Missing: 0\n",
            "Data is Updated\n",
            "_______________\n",
            "FARO\n",
            "_______________\n",
            "\n",
            "temperature-min\n",
            "Request Status: 200\n",
            "Original file already exists\n",
            "Creating validation file for  2020-05-04\n",
            "Days Missing: 0\n",
            "Data is Updated\n",
            "\n",
            "temperature-max\n",
            "Request Status: 200\n",
            "Original file already exists\n",
            "Creating validation file for  2020-05-04\n",
            "Days Missing: 0\n",
            "Data is Updated\n",
            "_______________\n",
            "LISBOA\n",
            "_______________\n",
            "\n",
            "temperature-min\n",
            "Request Status: 200\n",
            "Original file already exists\n",
            "Creating validation file for  2020-05-04\n",
            "Days Missing: 0\n",
            "Data is Updated\n",
            "\n",
            "temperature-max\n",
            "Request Status: 200\n",
            "Original file already exists\n",
            "Creating validation file for  2020-05-04\n",
            "Days Missing: 0\n",
            "Data is Updated\n",
            "_______________\n",
            "PORTO\n",
            "_______________\n",
            "\n",
            "temperature-min\n",
            "Request Status: 200\n",
            "Original file already exists\n",
            "Creating validation file for  2020-05-04\n",
            "Days Missing: 0\n",
            "Data is Updated\n",
            "\n",
            "temperature-max\n",
            "Request Status: 200\n",
            "Original file already exists\n",
            "Creating validation file for  2020-05-04\n",
            "Days Missing: 0\n",
            "Data is Updated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9B9NTWOt18Z",
        "colab_type": "text"
      },
      "source": [
        "# DEATHS IN PORTUGAL BY MONTH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whoc4XNasVV7",
        "colab_type": "text"
      },
      "source": [
        "Dados obtidos a partir do site do website PorData no link (https://www.pordata.pt/Portugal/%C3%93bitos+de+residentes+em+Portugal+total+e+por+m%C3%AAs+de+morte-3499)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICAxW1xUsTu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def deathsDataset():\n",
        "  data = pd.ExcelFile(datasetDeaths)\n",
        "  #read excel file \n",
        "  #sheet is Quadro\n",
        "  df = pd.read_excel(data, 'Quadro')\n",
        "  return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2nAyJj72n97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = deathsDataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrGBqbS3G1WU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getMeanMonths(df):\n",
        "  numYears = len(df.index)\n",
        "  total = list()\n",
        "  #for col in df.columns:\n",
        "  for col in range(len(df.columns)):\n",
        "    if df.columns[col] != 'Anos' and df.columns[col] != 'Total':\n",
        "      totalMonth = df[df.columns[col]].sum()\n",
        "      meanMonth = totalMonth/numYears\n",
        "      total.append([col-1, math.ceil(meanMonth)])\n",
        "  return total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VB47x8R2fP9",
        "colab_type": "code",
        "outputId": "3c03120e-223e-4550-e521-83af62f15f98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "deathsForMonth = list()\n",
        "deathsForMonth = getMeanMonths(df)\n",
        "print(deathsForMonth)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 11577], [2, 10238], [3, 9942], [4, 8703], [5, 8432], [6, 7925], [7, 8161], [8, 8152], [9, 7662], [10, 8334], [11, 8743], [12, 10286]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-C_v_z_2gKH",
        "colab_type": "text"
      },
      "source": [
        "# COVID-19 DATA ANALYSIS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SJ1HrmmA4Pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read csv file from url with raw data\n",
        "def read_dataset(url):\n",
        "\treturn pd.read_csv(url)\n",
        "\n",
        "#NOT FINISHED\n",
        "def prepare_dataset(df):\n",
        "  df_aux = df[['data', 'confirmados_arsnorte',\t'confirmados_arscentro',\t'confirmados_arslvt',\t'confirmados_arsalentejo',\t'confirmados_arsalgarve',\t'confirmados_acores',\t'confirmados_madeira']].copy()\n",
        "  df_aux['data'] = pd.to_datetime(df_aux['data'], format=\"%d-%m-%Y\")\n",
        "  df_aux.set_index('data')\n",
        "\n",
        "  df_death = df[['data', 'obitos']].copy()\n",
        "  print(df_death)\n",
        "  #df_death['data'] = pd.to_datetime(df_death['data'], format=\"%d-%m-%Y\").dt.to_period('M')\n",
        "  #df_death['month_year'] = pd.to_datetime(df_death['data']).dt.to_period('M')\n",
        "  #df_death.set_index('data', inplace=True)\n",
        "  # group = df_death.groupby(pd.PeriodIndex(data = df_death.month_year, freq='M'))\n",
        "  # result = group.sum()\n",
        "  #df_aux_death = df_death.copy()\n",
        "  # df_aux_death[['year', 'month']] = df_death['month_year'].apply(lambda x: pd.Series(str(x).split(\"-\")))\n",
        "  # yearI = df_aux_death.year.iloc[0]\n",
        "  # yearF = df_aux_death.year.iloc[-1]\n",
        "  dateI = '01-01-2020'\n",
        "  dateF = '31-12-2020'\n",
        "  print(dateI)\n",
        "  print(dateF)\n",
        "  #df_death['month_year'] = df_death['month_year'] + '-01'\n",
        "  #idx = pd.period_range(dateI, dateF, freq='M')\n",
        "  #print(idx)\n",
        "  #result.reindex(idx)\n",
        "  # groupby_day = df.groupby(pd.PeriodIndex(data=df.date, freq='D'))\n",
        "  # results = groupby_day.sum()\n",
        "\n",
        "  # idx = pd.date_range(min(df.date), max(df.date))\n",
        "  # results.reindex(idx, fill_value=0)\n",
        "  #df_death = df_death['month_year'].reindex(idx, fill_value=0)\n",
        "\n",
        "  return df_aux, df_death, dateI, dateF"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJIfCm5BMkT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#NOT FINISHED\n",
        "#STILL TESTING\n",
        "def filling_missing_dates(s, dI, dF):\n",
        "    idx = pd.period_range(dI, dF, freq='M')\n",
        "    print(idx)\n",
        "    #s.index = pd.PeriodIndex(s['data'], freq='M')\n",
        "    s = s.groupby(pd.PeriodIndex(data = s.data, freq='M')).sum()\n",
        "    print(s)\n",
        "    s = s.reindex(idx)\n",
        "    print(s)\n",
        "    s['data'] = s.index\n",
        "    s['obitos'] = s['obitos'].fillna(0)\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YNxhNEf2dyv",
        "colab_type": "code",
        "outputId": "c65350d4-3af8-4844-a7cb-6b5842b7f531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        }
      },
      "source": [
        "dataset = read_dataset(datasetCOVID)\n",
        "#print(dataset.head)\n",
        "\n",
        "df, df_death, dI, dF = prepare_dataset(dataset)\n",
        "\n",
        "data = filling_missing_dates(df_death, dI, dF)"
      ],
      "execution_count": 501,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          data  obitos\n",
            "0   26-02-2020       0\n",
            "1   27-02-2020       0\n",
            "2   28-02-2020       0\n",
            "3   29-02-2020       0\n",
            "4   01-03-2020       0\n",
            "..         ...     ...\n",
            "63  29-04-2020     973\n",
            "64  30-04-2020     989\n",
            "65  01-05-2020    1007\n",
            "66  02-05-2020    1023\n",
            "67  03-05-2020    1043\n",
            "\n",
            "[68 rows x 2 columns]\n",
            "01-01-2020\n",
            "31-12-2020\n",
            "PeriodIndex(['2020-01', '2020-02', '2020-03', '2020-04', '2020-05', '2020-06',\n",
            "             '2020-07', '2020-08', '2020-09', '2020-10', '2020-11', '2020-12'],\n",
            "            dtype='period[M]', freq='M')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-501-a635ce102583>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(df.head())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#print(final)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilling_missing_dates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_death\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-500-a44d37e7caf4>\u001b[0m in \u001b[0;36mfilling_missing_dates\u001b[0;34m(s, dI, dF)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#s.index = pd.PeriodIndex(s['data'], freq='M')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPeriodIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'M'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/period.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data, ordinal, freq, tz, dtype, copy, name, **fields)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;31m# don't pass copy here, since we copy later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperiod_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/arrays/period.py\u001b[0m in \u001b[0;36mperiod_array\u001b[0;34m(data, freq, copy)\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mPeriodArray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_datetime64\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mABCPeriodIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPeriodArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPeriodArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[0;31m# other iterable of some kind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/arrays/period.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, freq, dtype, copy)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Incorrect dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCPeriodIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Incorrect dtype"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raDZuZL0buek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_map(mapPath):\n",
        "  df_map = geopandas.read_file(mapPath)\n",
        "  return df_map\n",
        "\n",
        "\n",
        "def plot_Regions(df, df_map):\n",
        "  last_date = df.tail(1)\n",
        "  confirmed = {\n",
        "    \"Alentejo\": last_date[\"confirmados_arsalentejo\"].item(),\n",
        "    \"Algarve\": last_date[\"confirmados_arsalgarve\"].item(),\n",
        "    \"Centro\": last_date[\"confirmados_arscentro\"].item(),\n",
        "    \"Norte\": last_date[\"confirmados_arsnorte\"].item(), \n",
        "    \"RLVT\": last_date[\"confirmados_arslvt\"].item()\n",
        "  }\n",
        "  df_map[\"confirmados\"] = df_map[\"CCDR\"].map(confirmed)\n",
        "  fig, ax = plt.subplots(figsize=(15,6))\n",
        "  ax.set_title(f\"Casos Confirmados em Portugal Continental: {last_date['data'].item()}\", loc=\"left\", pad=12.0)\n",
        "  ax.axis('off')\n",
        "  df_map.plot(\n",
        "      column=VARIABLE, \n",
        "      cmap='Blues', \n",
        "      ax=ax,\n",
        "      legend=True,\n",
        "      linewidth=0.5,\n",
        "      edgecolor='0.8'\n",
        "  )\n",
        "  fig.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "#NOT COMPLETELY DONE\n",
        "def plot_Deaths(df_total, df):\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    #show the range of days since the first data\n",
        "    plt.plot(range(len(df_total)), df_total)\n",
        "    #plt.plot(df_total)\n",
        "    #plt.plot(data)\n",
        "    plt.title(\"Confirmed Cases of COVID-19\")\n",
        "    plt.ylabel('Cases')\n",
        "    plt.xlabel('Days')\n",
        "    plt.show()\n",
        "\n",
        "#NOT DONE\n",
        "def plot_Weather():\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUaktFhTcjxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df_map = get_map(mapPath)\n",
        "plot_Regions(df, df_map)\n",
        "plot_Deaths(deathsForMonth, df_death)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T6FspnsbwuY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Scale the data (values will be between -1 and 1) to increase the training speed and performance of the model\n",
        "#the normalization is between -1 and 1 because of the tanh function used to build the model\n",
        "def data_normalization(df, normalization_range=(-1, 1)):\n",
        "  scaler = MinMaxScaler(feature_range = normalization_range)\n",
        "  df[['confirmados_arsnorte']] = scaler.fit_transform(df[['confirmados_arsnorte']])\n",
        "  df[['confirmados_arscentro']] = scaler.fit_transform(df[['confirmados_arscentro']])\n",
        "  df[['confirmados_arslvt']] = scaler.fit_transform(df[['confirmados_arslvt']])\n",
        "  df[['confirmados_arsalentejo']] = scaler.fit_transform(df[['confirmados_arsalentejo']])\n",
        "  df[['confirmados_arsalgarve']] = scaler.fit_transform(df[['confirmados_arsalgarve']])\n",
        "  df[['confirmados_acores']] = scaler.fit_transform(df[['confirmados_acores']])\n",
        "  df[['confirmados_madeira']] = scaler.fit_transform(df[['confirmados_madeira']])\n",
        "  return scaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXWfkQN9-rM3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git add \"/content/AAII\"\n",
        "!git commit -m \"updated data folder\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdO86k0uNvQF",
        "colab_type": "code",
        "outputId": "e61f646b-0ca5-4893-a0f7-396f07612ec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!git push origin master"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Everything up-to-date\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFimfpQTAn-w",
        "colab_type": "text"
      },
      "source": [
        "BIBLIOGRAFIA\n",
        "https://medium.com/@navan0/how-to-push-files-into-github-from-google-colab-379fd0077aa8"
      ]
    }
  ]
}